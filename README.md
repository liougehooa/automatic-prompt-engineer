# Large Language Models Are Human-Level Prompt Engineers

This is based on [keirp/automatic_prompt_engineer](https://github.com/keirp/automatic_prompt_engineer.git) an implementation with Azure OpenAI. We upgraded LLM model and changed rating implementation methods to ELO for prompt rating comparison as previous method(which relies on echo) is [not working](https://community.openai.com/t/why-will-gpt-3-5-turbo-instruct-no-longer-support-echo-true-and-logprobs-1/404932) with latest LLM models.

# Abstract

By conditioning on natural language instructions, large language models (LLMs) have displayed impressive capabilities as
general-purpose computers. However, task performance depends significantly on the quality of the prompt used to steer
the model, and most effective prompts have been handcrafted by humans. Inspired by classical program synthesis and the
human approach to prompt engineering, we propose Automatic Prompt Engineer (APE) for automatic instruction generation
and selection. In our method, we treat the instruction as the “program,” optimized by searching over a pool of
instruction candidates proposed by an LLM in order to maximize a chosen score function. To evaluate the quality of the
selected instruction, we evaluate the zero-shot performance of another LLM following the selected instruction.
Experiments on 24 NLP tasks show that our automatically generated instructions outperform the prior LLM baseline by a
large margin and achieve better or comparable performance to the instructions generated by human annotators on 21/24
tasks. We conduct extensive qualitative and quantitative analyses to explore the performance of APE. We show that
APE-engineered prompts can be applied to steer models toward truthfulness and/or informativeness, as well as to improve
few-shot learning performance by simply prepending them to standard in-context learning prompts.

## Installation

To install APE, simply run:

```
pip install -e .
```

And add your AZURE_OPENAI_API_KEY with the following command:

```
os.environ['AZURE_OPENAI_API_KEY']=''
```

## Using `APE`

APE comes with one interfaces, the `simple_ape` function and a simplified version which takes care of most of the
configuration for you.

### Data :card_index:

Datasets in this codebase are represented using separate lists for inputs and outputs. For example, a dataset for words
and their antonyms can be written as:

```python
words = ["sane", "direct", "informally", "unpopular", "subtractive", "nonresidential",
         "inexact", "uptown", "incomparable", "powerful", "gaseous", "evenly", "formality",
         "deliberately", "off"]
antonyms = ["insane", "indirect", "formally", "popular", "additive", "residential",
            "exact", "downtown", "comparable", "powerless", "solid", "unevenly", "informality",
            "accidentally", "on"]
data = (words, antonyms)
```

### `simple_ape`

The `simple_ape` function takes the following arguments:

config_file='configs/bandits.yaml'

- `task` - A simple description of the task.
- `dataset` - The dataset to use (simple APE uses the same dataset for prompt generation, evaluation, and few-shot
- `config_file` - The path to the base configuration dataset.
    - `'configs/bandits.yaml'`: Configuration that uses the Upper Confidence Bound algorithm to save resources when evaluating prompts. Uses likelihood as its base evaluation method.

For more information on the `conf` dictionary, please refer to the annotations in `configs/bandits.yaml`.

`simple_ape` returns an evaluation result object. To get the best prompts and their associated scores from the evaluation result object, use
the `sorted()` method.

An example usage of this function would look like:

```python
from automatic_prompt_engineer import ape

words = ["sane", "direct", "informally", "unpopular", "subtractive", "nonresidential",
         "inexact", "uptown", "incomparable", "powerful", "gaseous", "evenly", "formality",
         "deliberately", "off"]
antonyms = ["insane", "indirect", "formally", "popular", "additive", "residential",
            "exact", "downtown", "comparable", "powerless", "solid", "unevenly", "informality",
            "accidentally", "on"]
            

result = ape.simple_ape(
    dataset=(words, antonyms),
    task='generate the related new word',
)
```

## Try it out! :eyes:

We provide a notebook for easily using APE: [`demo.ipynb`](demo.ipynb)


## For Unit test
Please run:
```python
pip install -r tests/requirements.txt
```